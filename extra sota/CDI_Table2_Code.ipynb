{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:38:32.492461Z",
     "start_time": "2024-04-03T19:38:32.487527Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "不同不同数据集+不同方法，需要\n",
    "修改： model_type  +  表格生成代码 + SDTR 方法--构造数据 + SDTR 方法--解析数据\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from scripts.create_data.TableNumerical import (InternEncoder_Inverse, InternEncoder, EntireEncoder,\n",
    "                                                EntireEncoder_Inverse, ParentChildEncoder, ParentChildEncoder_Inverse)\n",
    "\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.set_device(0)  # 指定GPU设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4155a99b78def171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:38:34.503632Z",
     "start_time": "2024-04-03T19:38:34.489385Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    " 0. Data  \n",
    "=========================================================================\n",
    "\"\"\"\n",
    "def run(model_type):\n",
    "      # 数据处理方式(base的网络结构)\n",
    "    experiments = 'CDI_Table2'\n",
    "    data_path = 'data/'+experiments+'.csv'\n",
    "    results_path = 'results/'+model_type+'/'+experiments\n",
    "    if not os.path.exists(results_path): # 检查并创建results文件夹（包括所有必要的中间文件夹）\n",
    "        os.makedirs(results_path)\n",
    "          \n",
    "    \n",
    "    df = pd.read_csv(data_path)  # 读取CSV文件\n",
    "    if \"Table2\" in experiments:\n",
    "        with open(data_path.replace('.csv', '_subclass_mapping.json')) as f:\n",
    "            subclass_mapping = json.load(f)\n",
    "        with open(data_path.replace('.csv', '_parent_child_mapping.json')) as f:\n",
    "            parent_child_mapping = json.load(f)\n",
    "    \n",
    "    # 编码方式\n",
    "    if \"Intern\" in model_type:\n",
    "        df, intern_encoder = InternEncoder(df)  # 每一列单独编码\n",
    "    elif \"Entire\" in model_type:\n",
    "        df, entire_encoder, vocab_per_column = EntireEncoder(df)  # 编码\n",
    "    elif \"ParentChild\" in model_type:\n",
    "        df, parent_child_encoder, label_encoder_list = ParentChildEncoder(df, parent_child_mapping, subclass_mapping)  # 父子类编码\n",
    "    \n",
    "    \n",
    "    num_rows, num_columns = df.shape\n",
    "    print(f\"原始数据集的表格尺寸为{num_rows}x{num_columns}\")\n",
    "    column_names, train_raw = df.columns, df.values  # 获取列名和数据\n",
    "    new_dataset_pd = df.values\n",
    "    batch_size = 32*3  # 3 channel for each column\n",
    "    \n",
    "    spatial_dimensional_transformation_inverse = False\n",
    "    spatial_dimensional_transformation = False\n",
    "    \n",
    "    if 'DimTrans' in model_type:\n",
    "        spatial_dimensional_transformation_inverse = True\n",
    "        spatial_dimensional_transformation = True\n",
    "    \n",
    "    \n",
    "    if \"RamCol\" in model_type:  # SDTR 方法 --- 构造数据\n",
    "        repeat = 10\n",
    "        num_columns = num_columns * repeat  #\n",
    "        df_torch = torch.from_numpy(df.to_numpy())\n",
    "        from scripts.SDTR import sdtr_transform\n",
    "        new_dataset_matrix, new_dataset_index = sdtr_transform(df_torch, num_columns)  # 100x12\n",
    "        new_dataset_pd = pd.DataFrame(new_dataset_matrix.detach().numpy())\n",
    "    \n",
    "        import csv\n",
    "        with open(data_path.replace('.csv','_list.txt'), mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for item in new_dataset_index:\n",
    "                writer.writerow([item])  # 将列表的每个元素作为单独的行写入文件\n",
    "    \n",
    "    # new_dataset_index = []\n",
    "    # with open(data_path.replace('.csv','_list.txt'), mode='r', newline='') as file:\n",
    "    #     reader = csv.reader(file)\n",
    "    #     for row in reader:\n",
    "    #         new_dataset_index.append(int(row[0])) # 将每行的第一个元素添加到列表中\n",
    "    \n",
    "    from scripts.ReadTable import read_table\n",
    "    # num_scaler, dataloader = read_table(df_values=new_dataset_pd)\n",
    "    num_scaler, dataloader = read_table(df_values=new_dataset_pd, batch_size=batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    =========================================================================\n",
    "    1. Model\n",
    "    =========================================================================\n",
    "    \"\"\"\n",
    "    from model.MLPSynthesizer import MLPSynthesizer  # MLP synthesizer model\n",
    "    from model.TransSynthesizer import TransSynthesizer  # Transformer synthesizer model\n",
    "    from model.UnetSynthesizer import UnetSynthesizer  # Unet synthesizer model\n",
    "    from model.BaseDiffusion import BaseDiffuser\n",
    "    if 'MLP' in model_type:\n",
    "        synthesizer_model = MLPSynthesizer(d_in=num_columns, hidden_layers=[128, 64, 128], activation='lrelu', dim_t=64)\n",
    "    if 'Unet' in model_type:\n",
    "        synthesizer_model = UnetSynthesizer(d_in=num_columns)  # 空间维度转换方法\n",
    "    if 'Transformer' in model_type:\n",
    "        synthesizer_model = TransSynthesizer(d_in=num_columns,  dim_t=num_columns)\n",
    "    diffuser_model = BaseDiffuser(total_steps=500, beta_start=1e-4, beta_end=0.02,device=device,\n",
    "                                  scheduler='linear')  # initialize the FinDiff base diffuser model\n",
    "    synthesizer_model = synthesizer_model.to(device)\n",
    "    \"\"\"\n",
    "    =========================================================================\n",
    "    2. Training\n",
    "    =========================================================================\n",
    "    \"\"\"\n",
    "    model_params_file = results_path+'/params.json'\n",
    "    model_weights_file = results_path+'/weights.pth'\n",
    "    \n",
    "    from scripts.Train import train_diffusion_model\n",
    "    train_epoch_losses = train_diffusion_model(dataloader, synthesizer_model, diffuser_model, device=device, epochs = 10, learning_rate = 1e-4,\n",
    "                                               spatial_dimensional_transformation = spatial_dimensional_transformation)\n",
    "    # from evaluation.LossCurve import visualization_loss_curve\n",
    "    # visualization_loss_curve(train_epoch_losses, savepath=results_path+'/TrainingEpochs_vs_MSEError.png')\n",
    "    from scripts.Train import save_model_and_params\n",
    "    save_model_and_params(synthesizer_model, diffuser_model, model_params_file, model_weights_file)  # 保存模型和超参数\n",
    "    \n",
    "    from scripts.Train import load_model_and_params\n",
    "    synthesizer_model, diffuser_model = load_model_and_params(model_params_file, model_weights_file, device=device)  # 加载模型和超参数\n",
    "    synthesizer_model=synthesizer_model.to(device)\n",
    "    \"\"\"\n",
    "    =========================================================================\n",
    "    3.  Generate Data\n",
    "    =========================================================================\n",
    "    \"\"\"\n",
    "    from scripts.Generate import generate_diffusion_model  # 生成数据 n_samples 必须是3的倍数\n",
    "    z_norm_upscaled = generate_diffusion_model(num_columns, num_scaler,  synthesizer_model, diffuser_model, device=device,\n",
    "                                               n_samples=999, sample_diffusion_steps=10,\n",
    "                                               spatial_dimensional_transformation_inverse=spatial_dimensional_transformation_inverse)\n",
    "    if \"RamCol\" in model_type:\n",
    "        # SDTR 方法 - - -  解析数据\n",
    "        from scripts.SDTR import sdtr_inverse_transform\n",
    "        z_norm_upscaled = sdtr_inverse_transform(z_norm_upscaled, new_dataset_index)  # 1000x4\n",
    "    \n",
    "    samples = pd.DataFrame(z_norm_upscaled, columns=column_names)  # convert generated samples to dataframe\n",
    "    samples = samples.round(0).astype(int)  # round the generated samples to integers\n",
    "    samples = samples.astype('category')\n",
    "    samples.to_csv(results_path+'/samples.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    =========================================================================\n",
    "    4. Evaluation : Generate Data\n",
    "    =========================================================================\n",
    "    \"\"\"\n",
    "    from evaluation.Visualization import visualize_table_distribution\n",
    "    # visualize_table_distribution(samples, savepath=results_path+'/samples_distribution.png')\n",
    "    if \"Intern\" in model_type:  # 每一列单独编码的解码\n",
    "        samples = InternEncoder_Inverse(samples, intern_encoder)\n",
    "        df = InternEncoder_Inverse(df, intern_encoder)\n",
    "    elif \"Entire\" in model_type:\n",
    "        samples = EntireEncoder_Inverse(samples, entire_encoder)  # 解码\n",
    "        df = EntireEncoder_Inverse(df, entire_encoder)  # 解码\n",
    "    elif \"ParentChild\" in model_type:\n",
    "        samples = ParentChildEncoder_Inverse(samples, parent_child_encoder, subclass_mapping, label_encoder_list)  # 父子类解码\n",
    "        df = ParentChildEncoder_Inverse(df, parent_child_encoder, subclass_mapping, label_encoder_list)\n",
    "    \n",
    "    from evaluation.Ratio_Difference import ratio_difference\n",
    "    average_difference = ratio_difference(df, samples, savepath=results_path+'/ratio_difference.txt')\n",
    "    \n",
    "    if \"Table2\" in experiments: # Table2 子类映射\n",
    "        from evaluation.Subclass_Error_Rate import subclass_error_rate\n",
    "        error_rate, error_count = subclass_error_rate(samples, subclass_mapping, parent_child_mapping, savepath=results_path+'/subclass_error_rate.txt')\n",
    "    \n",
    "    from evaluation.SVDevaluation import svd_evaluation\n",
    "    Column_Shapes, Column_Pair_Trends = svd_evaluation(real_data=df, synthetic_data=samples, savepath=results_path+'/svd_evaluation')\n",
    "\n",
    "    return average_difference, Column_Shapes, Column_Pair_Trends, error_rate, error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6246d6a22c42c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T19:39:57.825189Z",
     "start_time": "2024-04-03T19:38:36.714532Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 方法            | 列比例差异均值  | fidelity Column | fidelity row  |    错误分类率 |   错误数量 |\n",
      "| --------------------- | --------------- | --------------- | -------------- |-------------- |-------------- |\n",
      "|Diffusion_MLP_Intern | 0.0216 | 0.9678 | 0.8576 | 0.5155155155155156 | 515 |\n",
      "|Diffusion_MLP_Entire | 0.0702 | 0.9549 | 0.8158 | 0.5245245245245245 | 524 |\n",
      "|Diffusion_Transformer_Intern | 0.0933 | 0.9525 | 0.8454 | 0.5135135135135135 | 513 |\n",
      "|Diffusion_Transformer_Entire | 0.0839 | 0.9567 | 0.8170 | 0.5445445445445446 | 544 |\n",
      "|RamCol_MLP_Intern | 0.0076 | 0.9631 | 0.8546 | 0.5405405405405406 | 540 |\n",
      "|DimTrans_Unet_Intern | 0.0566 | 0.9592 | 0.8521 | 0.5155155155155156 | 515 |\n",
      "|DimTrans_Unet_Entire | 0.0720 | 0.9489 | 0.8110 | 0.5215215215215215 | 521 |\n",
      "|DimTrans_Unet_ParentChild | 0.0596 | 0.9607 | 0.8542 | 0.4774774774774775 | 477 |\n",
      "|RamCol+DimTrans_Unet_Intern | 0.0888 | 0.9699 | 0.8557 | 0.5275275275275275 | 527 |\n",
      "|RamCol+DimTrans_Unet_Entire | 0.0098 | 0.9487 | 0.8107 | 0.5295295295295295 | 529 |\n",
      "|RamCol+DimTrans_Unet_ParentChild | 0.0081 | 0.9637 | 0.8508 | 0.5135135135135135 | 513 |\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import io  \n",
    "from contextlib import redirect_stdout, redirect_stderr \n",
    "\n",
    "print(\"| 方法            | 列比例差异均值  | fidelity Column | fidelity row  |    错误分类率 |   错误数量 |\")\n",
    "print(\"| --------------------- | --------------- | --------------- | -------------- |-------------- |-------------- |\")\n",
    "\n",
    "f_out = io.StringIO()   # 使用一个 StringIO 对象来捕获输出  \n",
    "f_err = io.StringIO()  \n",
    "# \"RamCol_Transformer_Intern\"\n",
    "method_list = [\"Diffusion_MLP_Intern\", \"Diffusion_MLP_Entire\", \"Diffusion_Transformer_Intern\", \"Diffusion_Transformer_Entire\", \"RamCol_MLP_Intern\", \"DimTrans_Unet_Intern\", \"DimTrans_Unet_Entire\", \"DimTrans_Unet_ParentChild\",  \"RamCol+DimTrans_Unet_Intern\", \"RamCol+DimTrans_Unet_Entire\", \"RamCol+DimTrans_Unet_ParentChild\"]\n",
    "\n",
    "for model_type in method_list:\n",
    "    with redirect_stdout(f_out), redirect_stderr(f_err):  \n",
    "        average_difference, Column_Shapes, Column_Pair_Trends, error_rate, error_count = run(model_type)\n",
    "    print(f\"|{model_type} | {average_difference:.4f} | {Column_Shapes:.4f} | {Column_Pair_Trends:.4f} | {error_rate} | {error_count} |\")\n",
    "    \n",
    "# model_type = 'RamCol+DimTrans_Unet_ParentChild'\n",
    "# with redirect_stdout(f_out), redirect_stderr(f_err):  \n",
    "#     average_difference, Column_Shapes, Column_Pair_Trends, error_rate, error_count = run(model_type)\n",
    "# print(f\"|{model_type} | {average_difference:.4f} | {Column_Shapes:.4f} | {Column_Pair_Trends:.4f} | {error_rate} | {error_count} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99058e9a30bfa4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
